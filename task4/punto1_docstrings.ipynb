{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 1\n",
    "\n",
    "Selección de autores y libros: Elige tres autores cuyos trabajos estén disponibles en el Proyecto Gutenberg. Para cada uno de estos autores, selecciona y descarga cuidadosamente un mínimo de tres libros, sumando al menos nueve libros en total.\n",
    "Entrenamiento de embeddings: Utiliza estas obras literarias seleccionadas para entrenar word embeddings utilizando la biblioteca GENSIM y el modelo Word2Vec.\n",
    "\n",
    "•\tPrepara el conjunto de datos para el entrenamiento utilizando pasos adecuados de preprocesamiento de texto.\n",
    "•\tPrueba diferentes dimensionalidades de los embeddings (al menos 3) y guárdalos en disco utilizando los métodos apropiados de GENSIM con el siguiente formato de nombre de archivo:\n",
    "\n",
    "Books_<size_1>_<group_code>\n",
    "\n",
    "Books_<size_2>_<group_code>\n",
    "\n",
    "Books_<size_3>_<group_code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de bibliotecas y dependencias a usar en el ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "import string\n",
    "from sklearn.calibration import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementa una función que toma como entrada un texto de un libro de Gutenberg y elimina toda la información no relevante (introducciones, metadatos, nombres de autores y secciones innecesarias) para devolver solo el contenido principal del libro. Esto facilita el uso posterior del texto para tareas de procesamiento de lenguaje natural, ya que solo quedará el contenido que realmente importa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gutenberg_text(text):\n",
    "    # Eliminar el contenido antes de \"*** START OF THE PROJECT GUTENBERG EBOOK ***\"\n",
    "    text = re.sub(r'^.*?\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*', '', text, flags=re.DOTALL)\n",
    "    # Eliminar el contenido después de \"*** END OF THE PROJECT GUTENBERG EBOOK ***\"\n",
    "    text = re.sub(r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK.*?$', '', text, flags=re.DOTALL)\n",
    "    # Eliminar las menciones de autores específicos\n",
    "    authors = r'William Oberfield|James Branch Cabell|Wilhelm Raabe'\n",
    "    text = re.sub(r'(?i)\\b(?:' + authors + r')\\b.*?(?=\\n)', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'(Produced by.*?Distributed Proofreaders|Title:.*?Author:.*?Release date:.*?Language:.*?Credits:.*?Project Gutenberg Distributed Proofreaders)', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'(CONTENTS.*?THE AFTERWORD|BIBLIOGRAPHY|INDEX)', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementa una función para el preprocesaiento del texto, que toma como entrada un texto y lo preprocesa eliminando metadatos, números, puntuación y palabras vacías. Convierte el texto a minúsculas, lo tokeniza y aplica lematización para convertir las palabras a sus formas base. Este texto procesado es adecuado para ser utilizado en modelos de NLP para el manejo de embeddings que se quiere realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Limpiar el texto usando la función clean_gutenberg_text para eliminar secciones no deseadas\n",
    "    text = clean_gutenberg_text(text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenizar el texto, es decir, dividirlo en palabras individuales\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lematizar los tokens, eliminando las palabras que están en la lista de stopwords (palabras comunes como 'the', 'is', 'in')\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementa una función que recorre todos los archivos de texto (.txt) en un directorio, los abre, los lee, los preprocesa usando la función preprocess_text, y almacena las palabras procesadas (tokens) de cada archivo en una lista llamada corpus. El resultado es un conjunto de textos preprocesados listos para ser usados en modelos de procesamiento de lenguaje natural (NLP), donde cada archivo de texto se ha convertido en una lista de palabras procesadas y normalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_books_from_directory(directory):\n",
    "    corpus = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Abre el archivo en modo lectura con codificación UTF-8\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    global text\n",
    "                    text = f.read()\n",
    "                    # Preprocesa el texto (limpieza, tokenización, etc.) y devuelve los tokens\n",
    "                    tokens = preprocess_text(text)\n",
    "                    # Agrega los tokens procesados al corpus\n",
    "                    corpus.append(tokens)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta implementación, entrena modelos de Word2Vec con embeddings de 300, 400 y 500 dimensiones a partir de un corpus de textos preprocesados. El corpus se carga desde un directorio, y luego, en cada iteración, se entrena un modelo con un tamaño de embedding específico. Los modelos entrenados se guardan en archivos .model y se imprime una confirmación en la consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model with size 300 as 'Books_300_G4.model'\n",
      "Saved model with size 400 as 'Books_400_G4.model'\n",
      "Saved model with size 500 as 'Books_500_G4.model'\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "directory = 'books' \n",
    "corpus = load_books_from_directory(directory)\n",
    "\n",
    "# Lista de tamaños de embedding que se usarán para entrenar los modelos\n",
    "embedding_sizes = [300, 400, 500]\n",
    "group_code = \"G4\"\n",
    "\n",
    "for size in embedding_sizes:\n",
    "    # Crea un modelo Word2Vec con el corpus cargado\n",
    "    # vector_size: tamaño del vector de embedding\n",
    "    # window: número de palabras a considerar en el contexto a ambos lados\n",
    "    # min_count: ignora palabras que aparecen menos de 5 veces\n",
    "    # workers: número de núcleos de CPU usados para el entrenamiento\n",
    "    model = Word2Vec(sentences=corpus, vector_size=size, window=15, min_count=5, workers=4)\n",
    "    \n",
    "    model.save(f'Books_{size}_{group_code}.model')\n",
    "    print(f\"Saved model with size {size} as 'Books_{size}_{group_code}.model'\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
