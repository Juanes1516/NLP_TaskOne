{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. For the 20N dataset compare two classifiers NB and LR to identify the 20 different newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_20N = 'datasets/20news-18828/20news-18828/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Create your own processing pipeline for the task and justify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    ### Remover correos electrónicos\n",
    "    text = re.sub(r'[\\w\\d]+@[\\w\\d]+\\.[\\w\\d]+\\.?[\\w\\d]*\\.?[\\w\\d]*\\.?[\\w\\d]*\\.?[\\w\\d]*',' ',text)\n",
    "    #### Reemplazar números con la etiqueta NUM\n",
    "    text = re.sub(r'\\d+', 'NUM', text)\n",
    "    # Remover con un expresión regular carateres especiales (no palabras) excepto signos de puntuación.\n",
    "    text = re.sub(r'[^\\w\\s\\.,:;\\'\\?]', ' ', str(text))\n",
    "    # remover __ \n",
    "    text = re.sub(r'_+',' ',str(text))\n",
    "    # minúsculas\n",
    "    text = text.lower()\n",
    "    # stemming\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline_text(path20N):\n",
    "    categoria = []\n",
    "    texto = [] \n",
    "\n",
    "    for root, dirs, files in os.walk(path_20N):\n",
    "        for file in files:\n",
    "            categoria.append(root[root.rfind('/')+1:])\n",
    "            with open(os.path.join(root,file),'r',encoding='latin-1') as file: \n",
    "                text = file.read()\n",
    "                texto.append(preprocess_text(text))\n",
    "    return texto,categoria\n",
    "        \n",
    "\n",
    "textos,categorias = pipline_text(path20N=path_20N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Divide the dataset into training (60%), validation (10%) and test (30%).\n",
    "#### - Train NB and LR using the following vector representations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- tf (counts) representation (sklearn: CountVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tf = CountVectorizer(max_features=4000, stop_words=stopwords.words('english') )\n",
    "texto_features_tf = vectorizer_tf.fit_transform(textos).toarray()\n",
    "#Divide the dataset into training (60%), validation (10%) and test (30%).\n",
    "x_temp_tf, x_test_tf, y_temp_tf, y_test_tf = train_test_split(texto_features_tf,categorias,test_size=0.3)\n",
    "x_train_tf, x_val_tf, y_train_tf,y_val_tf = train_test_split(x_temp_tf,y_temp_tf,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - tfidf representation (sklearn: TfidfVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(max_features=4000, stop_words=stopwords.words('english'))\n",
    "texto_features_tfidf = vectorizer_tfidf.fit_transform(textos)\n",
    "#Divide the dataset into training (60%), validation (10%) and test (30%).\n",
    "x_temp_tfidf,x_test_tfidf,y_temp_tfidf,y_test_tfidf = train_test_split(texto_features_tfidf,categorias,test_size=0.3)\n",
    "x_train_tfidf,x_val_tfidf,y_train_tfidf,y_val_tfidf = train_test_split(x_temp_tfidf,y_temp_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▪ Train NB and LR using the following vector representations:\n",
    "▪ tf (counts) representation (sklearn: CountVectorizer).\n",
    "\n",
    "▪ tfidf representation (sklearn: TfidfVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf Naive Bayes accuracy score : 0.7905824039653035, params:{'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}\n",
      " tfidf Naive Bayes accuracy_score: 0.8350150469109577, params: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}\n"
     ]
    }
   ],
   "source": [
    "### Hacemos un entrenamiento tradicional para ver los resultados, haciendo un split de 70-30\n",
    "### y tomar este modelo como linea base de comparación\n",
    "nb_tf = MultinomialNB()\n",
    "nb_tf.fit(x_temp_tf,y_temp_tf)  ## x_temp_tf,y_temp_tf el 70% de los datos\n",
    "predictions_tf = nb_tf.predict(x_test_tf)\n",
    "print(f\" tf Naive Bayes accuracy score : {accuracy_score(y_test_tf,predictions_tf)}, params:{nb_tf.get_params()}\")\n",
    "\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(x_temp_tfidf,y_temp_tfidf) ## x_temp_tfidf,y_temp_tfidf el 70% de los datos\n",
    "predictions_tfidf = nb_tfidf.predict(x_test_tfidf)\n",
    "print(f\" tfidf Naive Bayes accuracy_score: {accuracy_score(y_test_tfidf,predictions_tfidf)}, params: {nb_tfidf.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf accuracy_score: 0.7643830766507347\n",
      "tfidf accuracy_score: 0.682421667551779\n"
     ]
    }
   ],
   "source": [
    "### Hacemos un entrenamiento tradicional para ver los resultados, haciendo un split de 70-30\n",
    "### y tomar este modelo como linea base de comparación\n",
    "logistic_model_SGD_tf = SGDClassifier(loss='log_loss', learning_rate='constant', eta0=0.0001)\n",
    "logistic_model_SGD_tf.fit(x_temp_tf,y_temp_tf)\n",
    "predictions_SGD_tf = logistic_model_SGD_tf.predict(x_test_tf)\n",
    "print(f'tf accuracy_score: {accuracy_score(y_test_tf,predictions_SGD_tf)}')\n",
    "\n",
    "logistic_model_SGD_tfidf = SGDClassifier(loss='log_loss', learning_rate='constant', eta0=0.0001)\n",
    "logistic_model_SGD_tfidf.fit(x_temp_tfidf,y_temp_tfidf)\n",
    "predictions_SGD_tfidf = logistic_model_SGD_tfidf.predict(x_test_tfidf)\n",
    "print(f'tfidf accuracy_score: {accuracy_score(y_test_tfidf,predictions_SGD_tfidf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mejores alpha: {'alpha': 0.1}, mejor score: 0.7996810703500276\n",
      "tf cross validations accuracy_score: 0.7976633032395114\n"
     ]
    }
   ],
   "source": [
    "## para Naive Bayes buscamos el alpha más adecuado para suavizar con laplace, \n",
    "## usualmente alpha=1 y usamos el método de crosvalidation.\n",
    "grid = {'alpha':[0.001,0.01,0.1,1,10]}  # buscamos sobre estos alphas para ver cuál es mejor\n",
    "nb_tf_crossval = MultinomialNB()\n",
    "grid_search_tf_crossval = GridSearchCV(nb_tf_crossval,grid, cv=10)   # 10 folders\n",
    "grid_search_tf_crossval.fit(x_temp_tf,y_temp_tf)\n",
    "print(f\"mejores alpha: {grid_search_tf_crossval.best_params_}, mejor score: {grid_search_tf_crossval.best_score_}\")\n",
    "predictions_nb_tf_crossval = grid_search_tf_crossval.predict(x_test_tf)\n",
    "print(f\"tf cross validations accuracy_score: {accuracy_score(y_test_tf,predictions_nb_tf_crossval)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mejores alpha: {'alpha': 0.1}, mejor score: 0.8524920987714065\n",
      "tfid cross validations accuracy_score: 0.8534253850238981\n"
     ]
    }
   ],
   "source": [
    "## para Naive Bayes buscamos el alpha más adecuado para suavizar con laplace, \n",
    "## usualmente alpha=1 y usamos el método de crosvalidation.\n",
    "grid = {'alpha':[0.001,0.01,0.1,1,10]}  # buscamos sobre estos alphas para ver cuál es mejor\n",
    "nb_tfid_crossval = MultinomialNB()\n",
    "grid_search_tfid_crossval = GridSearchCV(nb_tfid_crossval,grid, cv=10,scoring='accuracy')   # 10 folders\n",
    "grid_search_tfid_crossval.fit(x_temp_tfidf,y_temp_tfidf)\n",
    "print(f\"mejores alpha: {grid_search_tfid_crossval.best_params_}, mejor score: {grid_search_tfid_crossval.best_score_}\")\n",
    "predictions_nb_tfid_crossval = grid_search_tfid_crossval.predict(x_test_tfidf)\n",
    "print(f\"tfid cross validations accuracy_score: {accuracy_score(y_test_tfidf,predictions_nb_tfid_crossval)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid_SGVclassifier = {\n",
    "#     'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "#     'eta0': [0.001, 0.01, 0.1, 1],  # Only relevant for 'constant', 'invscaling', and 'adaptive'\n",
    "#     'alpha': [0.0001, 0.001, 0.01, 0.1]  # Regularization term\n",
    "# }\n",
    "\n",
    "# sgvclas_tf_crv = SGDClassifier(loss='log_loss',max_iter=1000,tol=0.001)\n",
    "# grid_search_sgvclas_tf_crv = GridSearchCV(sgvclas_tf_crv,param_grid_SGVclassifier,cv=10,scoring='accuracy')\n",
    "# grid_search_sgvclas_tf_crv.fit(x_temp_tf,y_temp_tf) # 70% data\n",
    "# print(f'mejores params: {grid_search_sgvclas_tf_crv.best_params_}, mejor score: {grid_search_sgvclas_tf_crv.best_score_}')\n",
    "# predictions_sgvclass_tf_crv = grid_search_sgvclas_tf_crv.predict(x_test_tf)\n",
    "# print(f'tf score sgv crossv: {accuracy_score(y_test_tf,predictions_sgvclass_tf_crv)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_SGVclassifier = {\n",
    "    'learning_rate': ['constant'],\n",
    "    'eta0': [0.001, 0.01, 0.1, 1],  \n",
    "}\n",
    "\n",
    "sgvclas_tf_crv = SGDClassifier(loss='log_loss',max_iter=1000,tol=0.001)\n",
    "grid_search_sgvclas_tf_crv = GridSearchCV(sgvclas_tf_crv,param_grid_SGVclassifier,cv=10,scoring='accuracy')\n",
    "grid_search_sgvclas_tf_crv.fit(x_temp_tf,y_temp_tf) # 70% data\n",
    "print(f'mejores params: {grid_search_sgvclas_tf_crv.best_params_}, mejor score: {grid_search_sgvclas_tf_crv.best_score_}')\n",
    "predictions_sgvclass_tf_crv = grid_search_sgvclas_tf_crv.predict(x_test_tf)\n",
    "print(f'tf score sgv crossv: {accuracy_score(y_test_tf,predictions_sgvclass_tf_crv)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos una función para entrenamiento por épocas para \n",
    "# el SGDClassifier evitando sobreajuste\n",
    "\n",
    "def SGD_epochs(xtrain,ytrain,xval,yval,num_epochs,tolerance=0.1,stop=5):\n",
    "    best_accuracy = 0\n",
    "    num_noimprovements = 0\n",
    "    best_model = None\n",
    "    clases = np.unique(ytrain)\n",
    "    model = SGDClassifier(loss='log_loss',learning_rate='constant',eta0=0.0001) \n",
    "    for i in range(num_epochs):\n",
    "        # print(f'época: {i}') \n",
    "        model.fit(xtrain,ytrain)  # entrenar con pequeños lotes de datos \n",
    "        predictions = model.predict(xval)\n",
    "\n",
    "        accuracy = accuracy_score(yval,predictions)\n",
    "\n",
    "        if accuracy > best_accuracy*(1+tolerance):\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model \n",
    "            num_noimprovements = 0 \n",
    "        else: \n",
    "            num_noimprovements += 1\n",
    "\n",
    "        print(f'best accuracy: {best_accuracy},epoch: {i}')\n",
    "        if num_noimprovements>(stop-2):\n",
    "            print(f'existe posible sobreajuste, época: {i}')\n",
    "            return  best_model\n",
    "        \n",
    "    return best_model\n",
    "\n",
    "model_tf = SGD_epochs(x_train_tf,y_train_tf,x_val_tf,y_val_tf,100)\n",
    "predictions_tf2 = model_tf.predict(x_test_tf)\n",
    "print(f'tf accuracy: {accuracy_score(y_test_tf,predictions_tf2)}')\n",
    "model_tf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'average': False,\n",
       " 'class_weight': None,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 0.1,\n",
       " 'eta0': 0.0001,\n",
       " 'fit_intercept': True,\n",
       " 'l1_ratio': 0.15,\n",
       " 'learning_rate': 'constant',\n",
       " 'loss': 'log_loss',\n",
       " 'max_iter': 1000,\n",
       " 'n_iter_no_change': 5,\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'tol': 0.001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf.get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
